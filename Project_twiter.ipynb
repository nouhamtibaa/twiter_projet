{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet: Fouille de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réaliser par: Nouha MTIBAA-3DNI_G1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thème: Classification des Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectifs: \n",
    "* Maitriser l’API de twitter pour l’extraction des tweets\n",
    "* Maitriser la partie NLP (naturallanguageprocessing) avec NLTK en Python\n",
    "* Appliquer les principes de nettoyage des données\n",
    "* Classer les tweets: regrouper ensemble les tweets qui sont similaires.C’est une étape qui peut être considérée comme une étape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des bibliothéques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import tweepy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in c:\\users\\nouha\\anaconda\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\nouha\\anaconda\\lib\\site-packages (from tweepy) (1.12.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\users\\nouha\\anaconda\\lib\\site-packages (from tweepy) (2.22.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\nouha\\anaconda\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\nouha\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nouha\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\nouha\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\nouha\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\nouha\\anaconda\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\nouha\\anaconda\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\nouha\\anaconda\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\nouha\\anaconda\\lib\\site-packages (from pandas) (1.16.4)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\nouha\\anaconda\\lib\\site-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\nouha\\anaconda\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nouha\\anaconda\\lib\\site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On doit récupérer les Tweets à partir de Twitter en utilisant l’API de twitter et les sauvgarder dans un fichier .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Twitter HashTag to search for\n"
     ]
    }
   ],
   "source": [
    "def scrape_function(words, date_since, numtweet): \n",
    "      \n",
    "    # Creating DataFrame using pandas \n",
    "    db = pd.DataFrame(columns=['username', 'description', 'location', 'following', \n",
    "                               'followers', 'totaltweets', 'retweetcount', 'text', 'hashtags']) \n",
    "      \n",
    "    # We are using .Cursor() to search through twitter for the required tweets. \n",
    "    # The number of tweets can be restricted using .items(number of tweets) \n",
    "    tweets = tweepy.Cursor(api.search, q=words, lang=\"en\", \n",
    "                           since=date_since, tweet_mode='extended').items(numtweet) \n",
    "     \n",
    "    # .Cursor() returns an iterable object. Each item in  \n",
    "    # the iterator has various attributes that you can access to  \n",
    "    # get information about each tweet \n",
    "    list_tweets = [tweet for tweet in tweets] \n",
    "      \n",
    "    # Counter to maintain Tweet Count \n",
    "    i = 1  \n",
    "      \n",
    "    # we will iterate over each tweet in the list for extracting information about each tweet \n",
    "    for tweet in list_tweets: \n",
    "        username = tweet.user.screen_name \n",
    "        description = tweet.user.description \n",
    "        location = tweet.user.location \n",
    "        following = tweet.user.friends_count \n",
    "        followers = tweet.user.followers_count \n",
    "        totaltweets = tweet.user.statuses_count \n",
    "        retweetcount = tweet.retweet_count \n",
    "        hashtags = tweet.entities['hashtags'] \n",
    "          \n",
    "        # Retweets can be distinguished by a retweeted_status attribute, \n",
    "        # in case it is an invalid reference, except block will be executed \n",
    "        try: \n",
    "            text = tweet.retweeted_status.full_text \n",
    "        except AttributeError: \n",
    "            text = tweet.full_text \n",
    "        hashtext = list() \n",
    "        for j in range(0, len(hashtags)): \n",
    "            hashtext.append(hashtags[j]['text']) \n",
    "          \n",
    "        # Here we are appending all the extracted information in the DataFrame \n",
    "        ith_tweet = [username, description, location, following, \n",
    "                     followers, totaltweets, retweetcount, text, hashtext] \n",
    "        db.loc[len(db)] = ith_tweet \n",
    "          \n",
    "        # Function call to print tweet data on screen \n",
    "        printtweetdata(i, ith_tweet) \n",
    "        i = i+1\n",
    "    filename = 'tweets.csv'\n",
    "      \n",
    "    # we will save our database as a CSV file. \n",
    "    db.to_csv(filename) \n",
    "  \n",
    "  \n",
    "if __name__ == '__main__': \n",
    "      \n",
    "    # Enter your own credentials obtained  \n",
    "    # from your developer account \n",
    "    consumer_key = \"ry9GMoJRUCdsHo89bkpVosu8T\"\n",
    "    consumer_secret = \"y9Yy5X6RE3n6hEogGhVYCmGDKgDPhWuQB8sexc61xKH01Xv6MA\"\n",
    "    access_key = \"1330861473481052162-NXO8xkGNxf0g6A7wbOjOfMPKZrfUvT\"\n",
    "    access_secret = \"20Ye373RjvjjMfpbPHAW6kbwJURRbqRaQsnMYFBCrZpyw\"\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "    auth.set_access_token(access_key, access_secret) \n",
    "    api = tweepy.API(auth,wait_on_rate_limit=True) \n",
    "      \n",
    "    # Enter Hashtag and initial date \n",
    "    print(\"Enter Twitter HashTag to search for\") \n",
    "    words = input() \n",
    "    print(\"Enter Date since The Tweets are required in yyyy-mm--dd\") \n",
    "    date_since = input() \n",
    "      \n",
    "    # number of tweets you want to extract in one run \n",
    "    numtweet =2\n",
    "    scrape_function(words, date_since, numtweet) \n",
    "    print('Scraping has completed!') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On va afficher les dates de chaque tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display data of each tweet \n",
    "def printtweetdata(n, ith_tweet): \n",
    "    print() \n",
    "    print(f\"Tweet {n}:\") \n",
    "    print(f\"Username:{ith_tweet[0]}\") \n",
    "    print(f\"Description:{ith_tweet[1]}\") \n",
    "    print(f\"Location:{ith_tweet[2]}\") \n",
    "    print(f\"Following Count:{ith_tweet[3]}\") \n",
    "    print(f\"Follower Count:{ith_tweet[4]}\") \n",
    "    print(f\"Total Tweets:{ith_tweet[5]}\") \n",
    "    print(f\"Retweet Count:{ith_tweet[6]}\") \n",
    "    print(f\"Tweet Text:{ith_tweet[7]}\") \n",
    "    print(f\"Hashtags Used:{ith_tweet[8]}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### on va lire les deux fichier dataset puis les concaténer ensemble dans un autre fichier df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4985, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>username</th>\n",
       "      <th>description</th>\n",
       "      <th>location</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>totaltweets</th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>DatGladiatah</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134</td>\n",
       "      <td>14</td>\n",
       "      <td>2226</td>\n",
       "      <td>0</td>\n",
       "      <td>@TulsiGabbard the biological essentialism of T...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>EliHayes_</td>\n",
       "      <td>ECU Alumnus 🏴‍☠️ | 910</td>\n",
       "      <td>North Carolina, USA</td>\n",
       "      <td>84</td>\n",
       "      <td>76</td>\n",
       "      <td>4239</td>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>braincelltwo</td>\n",
       "      <td>Taking it well</td>\n",
       "      <td>Art by @sloppjockey</td>\n",
       "      <td>988</td>\n",
       "      <td>4496</td>\n",
       "      <td>54644</td>\n",
       "      <td>0</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>WuanChill</td>\n",
       "      <td>#MCFC #FlyEaglesFly. This is a Anthony Martial...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>279</td>\n",
       "      <td>137</td>\n",
       "      <td>17911</td>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>schweitz_ay</td>\n",
       "      <td>This guy gets it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>425</td>\n",
       "      <td>57</td>\n",
       "      <td>1027</td>\n",
       "      <td>0</td>\n",
       "      <td>@DanSoder @katienolan that Andre AD read made ...</td>\n",
       "      <td>['sports']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      username  \\\n",
       "0           0  DatGladiatah   \n",
       "1           1     EliHayes_   \n",
       "2           2  braincelltwo   \n",
       "3           3     WuanChill   \n",
       "4           4   schweitz_ay   \n",
       "\n",
       "                                         description             location  \\\n",
       "0                                                NaN                  NaN   \n",
       "1                             ECU Alumnus 🏴‍☠️ | 910  North Carolina, USA   \n",
       "2                                     Taking it well  Art by @sloppjockey   \n",
       "3  #MCFC #FlyEaglesFly. This is a Anthony Martial...                  NaN   \n",
       "4                                   This guy gets it                  NaN   \n",
       "\n",
       "   following  followers  totaltweets  retweetcount  \\\n",
       "0        134         14         2226             0   \n",
       "1         84         76         4239             1   \n",
       "2        988       4496        54644             0   \n",
       "3        279        137        17911             1   \n",
       "4        425         57         1027             0   \n",
       "\n",
       "                                                text    hashtags  \n",
       "0  @TulsiGabbard the biological essentialism of T...          []  \n",
       "1  mighty funny how nobody in black sports media ...          []  \n",
       "2  Tht time I was put in a gc full of barstool sp...          []  \n",
       "3  mighty funny how nobody in black sports media ...          []  \n",
       "4  @DanSoder @katienolan that Andre AD read made ...  ['sports']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1= pd.read_csv('tweets1.csv')\n",
    "df=pd.read_csv('tweets.csv')\n",
    "df_final=pd.concat([df, df_1], ignore_index=True)\n",
    "print(df_final.shape)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2985, 10)\n",
      "(2000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On va éliminer les colonnes dans la liste de to_drop du dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@TulsiGabbard the biological essentialism of T...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@DanSoder @katienolan that Andre AD read made ...</td>\n",
       "      <td>['sports']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retweetcount                                               text    hashtags\n",
       "0             0  @TulsiGabbard the biological essentialism of T...          []\n",
       "1             1  mighty funny how nobody in black sports media ...          []\n",
       "2             0  Tht time I was put in a gc full of barstool sp...          []\n",
       "3             1  mighty funny how nobody in black sports media ...          []\n",
       "4             0  @DanSoder @katienolan that Andre AD read made ...  ['sports']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_drop = ['Unnamed: 0',\n",
    "           'username',\n",
    "           'description',\n",
    "           'location',\n",
    "           'following',\n",
    "           'followers',\n",
    "           'totaltweets']\n",
    "#Eliminer les colonnes dans la liste de to_drop du dataframe df\n",
    "df.drop(to_drop, inplace=True, axis=1)\n",
    "#Afficherle résultat\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitement\n",
    "\n",
    "Les tweets contiennent des objets inutiles tels que des hashtags, des mentions, des liens et des signes de ponctuation qui peuvent affecter les performances d'un algorithme et doivent donc être supprimés. Tous les textes sont convertis en minuscules pour éviter que les algorithmes n'interprètent les mêmes mots avec des cas différents comme différents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string , re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "string.punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On va maintenant supprimez les hashtags, les mentions et les caractères indésirables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Tweet_punctuation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@TulsiGabbard the biological essentialism of T...</td>\n",
       "      <td>[]</td>\n",
       "      <td>TulsiGabbard the biological essentialism of TE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@DanSoder @katienolan that Andre AD read made ...</td>\n",
       "      <td>['sports']</td>\n",
       "      <td>DanSoder katienolan that Andre AD read made me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>@oCHiCAx I agree. I believe in trans rights 2 ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>oCHiCAx I agree I believe in trans rights  but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42</td>\n",
       "      <td>constantly making fun of the wnba and women's ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>constantly making fun of the wnba and womens s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Funny how things come full circle, @EmilyJones...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Funny how things come full circle EmilyJonesMc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42</td>\n",
       "      <td>constantly making fun of the wnba and women's ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>constantly making fun of the wnba and womens s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>Winter sports are funny, especially when you’r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Winter sports are funny especially when you’re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retweetcount                                               text  \\\n",
       "0             0  @TulsiGabbard the biological essentialism of T...   \n",
       "1             1  mighty funny how nobody in black sports media ...   \n",
       "2             0  Tht time I was put in a gc full of barstool sp...   \n",
       "3             1  mighty funny how nobody in black sports media ...   \n",
       "4             0  @DanSoder @katienolan that Andre AD read made ...   \n",
       "5             0  @oCHiCAx I agree. I believe in trans rights 2 ...   \n",
       "6            42  constantly making fun of the wnba and women's ...   \n",
       "7             1  Funny how things come full circle, @EmilyJones...   \n",
       "8            42  constantly making fun of the wnba and women's ...   \n",
       "9             3  Winter sports are funny, especially when you’r...   \n",
       "\n",
       "     hashtags                                  Tweet_punctuation  \n",
       "0          []  TulsiGabbard the biological essentialism of TE...  \n",
       "1          []  mighty funny how nobody in black sports media ...  \n",
       "2          []  Tht time I was put in a gc full of barstool sp...  \n",
       "3          []  mighty funny how nobody in black sports media ...  \n",
       "4  ['sports']  DanSoder katienolan that Andre AD read made me...  \n",
       "5          []  oCHiCAx I agree I believe in trans rights  but...  \n",
       "6          []  constantly making fun of the wnba and womens s...  \n",
       "7          []  Funny how things come full circle EmilyJonesMc...  \n",
       "8          []  constantly making fun of the wnba and womens s...  \n",
       "9          []  Winter sports are funny especially when you’re...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ponctuation_sup(text_entree):\n",
    "    text_entree = \"\".join([char for char in text_entree if char not in string.punctuation])\n",
    "    text_entree = re.sub('[0-9]+', '', text_entree)\n",
    "    \n",
    "    return text_entree\n",
    "\n",
    "df['Tweet_punctuation'] = df['text'].apply(lambda x: ponctuation_sup(x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation, lemmatisation et suppression des mots vides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### La tokenisation cherche à transformer un texte en une série de tokens individuels. Dans l’idée, chaque token représente un mot, et identifier des mots semble être une tâche relativement simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Tweet_punctuation</th>\n",
       "      <th>Tweet_no_mot</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@TulsiGabbard the biological essentialism of T...</td>\n",
       "      <td>[]</td>\n",
       "      <td>TulsiGabbard the biological essentialism of TE...</td>\n",
       "      <td>TulsiGabbard biological essentialism TERFs fun...</td>\n",
       "      <td>[tulsigabbard, biological, essentialism, terfs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>mighty funny nobody black sports media standin...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>time full barstool sports type funny accounts ...</td>\n",
       "      <td>[time, full, barstool, sports, type, funny, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>mighty funny nobody black sports media standin...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@DanSoder @katienolan that Andre AD read made ...</td>\n",
       "      <td>['sports']</td>\n",
       "      <td>DanSoder katienolan that Andre AD read made me...</td>\n",
       "      <td>DanSoder katienolan that Andre read made swerv...</td>\n",
       "      <td>[dansoder, katienolan, that, andre, read, made...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retweetcount                                               text  \\\n",
       "0             0  @TulsiGabbard the biological essentialism of T...   \n",
       "1             1  mighty funny how nobody in black sports media ...   \n",
       "2             0  Tht time I was put in a gc full of barstool sp...   \n",
       "3             1  mighty funny how nobody in black sports media ...   \n",
       "4             0  @DanSoder @katienolan that Andre AD read made ...   \n",
       "\n",
       "     hashtags                                  Tweet_punctuation  \\\n",
       "0          []  TulsiGabbard the biological essentialism of TE...   \n",
       "1          []  mighty funny how nobody in black sports media ...   \n",
       "2          []  Tht time I was put in a gc full of barstool sp...   \n",
       "3          []  mighty funny how nobody in black sports media ...   \n",
       "4  ['sports']  DanSoder katienolan that Andre AD read made me...   \n",
       "\n",
       "                                        Tweet_no_mot  \\\n",
       "0  TulsiGabbard biological essentialism TERFs fun...   \n",
       "1  mighty funny nobody black sports media standin...   \n",
       "2  time full barstool sports type funny accounts ...   \n",
       "3  mighty funny nobody black sports media standin...   \n",
       "4  DanSoder katienolan that Andre read made swerv...   \n",
       "\n",
       "                                     Tweet_tokenized  \n",
       "0  [tulsigabbard, biological, essentialism, terfs...  \n",
       "1  [mighty, funny, nobody, black, sports, media, ...  \n",
       "2  [time, full, barstool, sports, type, funny, ac...  \n",
       "3  [mighty, funny, nobody, black, sports, media, ...  \n",
       "4  [dansoder, katienolan, that, andre, read, made...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['Tweet_no_mot'] = df['Tweet_punctuation'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "def tokenization(text):\n",
    "    text = re.split(' ', text)\n",
    "    return text\n",
    "\n",
    "df['Tweet_tokenized'] = df['Tweet_no_mot'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Tweet_punctuation</th>\n",
       "      <th>Tweet_no_mot</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@TulsiGabbard the biological essentialism of T...</td>\n",
       "      <td>[]</td>\n",
       "      <td>TulsiGabbard the biological essentialism of TE...</td>\n",
       "      <td>TulsiGabbard biological essentialism TERFs fun...</td>\n",
       "      <td>[tulsigabbard, biological, essentialism, terfs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>mighty funny nobody black sports media standin...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>time full barstool sports type funny accounts ...</td>\n",
       "      <td>[time, full, barstool, sports, type, funny, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>mighty funny nobody black sports media standin...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@DanSoder @katienolan that Andre AD read made ...</td>\n",
       "      <td>['sports']</td>\n",
       "      <td>DanSoder katienolan that Andre AD read made me...</td>\n",
       "      <td>DanSoder katienolan that Andre read made swerv...</td>\n",
       "      <td>[dansoder, katienolan, that, andre, read, made...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retweetcount                                               text  \\\n",
       "0             0  @TulsiGabbard the biological essentialism of T...   \n",
       "1             1  mighty funny how nobody in black sports media ...   \n",
       "2             0  Tht time I was put in a gc full of barstool sp...   \n",
       "3             1  mighty funny how nobody in black sports media ...   \n",
       "4             0  @DanSoder @katienolan that Andre AD read made ...   \n",
       "\n",
       "     hashtags                                  Tweet_punctuation  \\\n",
       "0          []  TulsiGabbard the biological essentialism of TE...   \n",
       "1          []  mighty funny how nobody in black sports media ...   \n",
       "2          []  Tht time I was put in a gc full of barstool sp...   \n",
       "3          []  mighty funny how nobody in black sports media ...   \n",
       "4  ['sports']  DanSoder katienolan that Andre AD read made me...   \n",
       "\n",
       "                                        Tweet_no_mot  \\\n",
       "0  TulsiGabbard biological essentialism TERFs fun...   \n",
       "1  mighty funny nobody black sports media standin...   \n",
       "2  time full barstool sports type funny accounts ...   \n",
       "3  mighty funny nobody black sports media standin...   \n",
       "4  DanSoder katienolan that Andre read made swerv...   \n",
       "\n",
       "                                     Tweet_tokenized  \n",
       "0  [tulsigabbard, biological, essentialism, terfs...  \n",
       "1  [mighty, funny, nobody, black, sports, media, ...  \n",
       "2  [time, full, barstool, sports, type, funny, ac...  \n",
       "3  [mighty, funny, nobody, black, sports, media, ...  \n",
       "4  [dansoder, katienolan, that, andre, read, made...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Tweet_tokenized'] = df['Tweet_no_mot'].apply(lambda x: tokenization(x.lower()))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Tweet_punctuation</th>\n",
       "      <th>Tweet_no_mot</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@TulsiGabbard the biological essentialism of T...</td>\n",
       "      <td>[]</td>\n",
       "      <td>TulsiGabbard the biological essentialism of TE...</td>\n",
       "      <td>TulsiGabbard biological essentialism TERFs fun...</td>\n",
       "      <td>[tulsigabbard, biological, essentialism, terfs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>mighty funny nobody black sports media standin...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>time full barstool sports type funny accounts ...</td>\n",
       "      <td>[time, full, barstool, sports, type, funny, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>mighty funny nobody black sports media standin...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@DanSoder @katienolan that Andre AD read made ...</td>\n",
       "      <td>['sports']</td>\n",
       "      <td>DanSoder katienolan that Andre AD read made me...</td>\n",
       "      <td>DanSoder katienolan that Andre read made swerv...</td>\n",
       "      <td>[dansoder, katienolan, that, andre, read, made...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retweetcount                                               text  \\\n",
       "0             0  @TulsiGabbard the biological essentialism of T...   \n",
       "1             1  mighty funny how nobody in black sports media ...   \n",
       "2             0  Tht time I was put in a gc full of barstool sp...   \n",
       "3             1  mighty funny how nobody in black sports media ...   \n",
       "4             0  @DanSoder @katienolan that Andre AD read made ...   \n",
       "\n",
       "     hashtags                                  Tweet_punctuation  \\\n",
       "0          []  TulsiGabbard the biological essentialism of TE...   \n",
       "1          []  mighty funny how nobody in black sports media ...   \n",
       "2          []  Tht time I was put in a gc full of barstool sp...   \n",
       "3          []  mighty funny how nobody in black sports media ...   \n",
       "4  ['sports']  DanSoder katienolan that Andre AD read made me...   \n",
       "\n",
       "                                        Tweet_no_mot  \\\n",
       "0  TulsiGabbard biological essentialism TERFs fun...   \n",
       "1  mighty funny nobody black sports media standin...   \n",
       "2  time full barstool sports type funny accounts ...   \n",
       "3  mighty funny nobody black sports media standin...   \n",
       "4  DanSoder katienolan that Andre read made swerv...   \n",
       "\n",
       "                                     Tweet_tokenized  \n",
       "0  [tulsigabbard, biological, essentialism, terfs...  \n",
       "1  [mighty, funny, nobody, black, sports, media, ...  \n",
       "2  [time, full, barstool, sports, type, funny, ac...  \n",
       "3  [mighty, funny, nobody, black, sports, media, ...  \n",
       "4  [dansoder, katienolan, that, andre, read, made...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vas utiliser la bibliothèque NLTK pour effectuer une analyse de chaque tweet et le transformer en un ensemble de mots en suivant les différentes étapes de base du processus NLP (Natural Language Processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nouha\\anaconda\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: six in c:\\users\\nouha\\anaconda\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nouha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Tweet_punctuation</th>\n",
       "      <th>Tweet_no_mot</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@TulsiGabbard the biological essentialism of T...</td>\n",
       "      <td>[]</td>\n",
       "      <td>TulsiGabbard the biological essentialism of TE...</td>\n",
       "      <td>TulsiGabbard biological essentialism TERFs fun...</td>\n",
       "      <td>[tulsigabbard, biological, essentialism, terfs...</td>\n",
       "      <td>[tulsigabbard, biological, essentialism, terfs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>mighty funny nobody black sports media standin...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>time full barstool sports type funny accounts ...</td>\n",
       "      <td>[time, full, barstool, sports, type, funny, ac...</td>\n",
       "      <td>[time, full, barstool, sports, type, funny, ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>mighty funny nobody black sports media standin...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@DanSoder @katienolan that Andre AD read made ...</td>\n",
       "      <td>['sports']</td>\n",
       "      <td>DanSoder katienolan that Andre AD read made me...</td>\n",
       "      <td>DanSoder katienolan that Andre read made swerv...</td>\n",
       "      <td>[dansoder, katienolan, that, andre, read, made...</td>\n",
       "      <td>[dansoder, katienolan, andre, read, made, swer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>@oCHiCAx I agree. I believe in trans rights 2 ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>oCHiCAx I agree I believe in trans rights  but...</td>\n",
       "      <td>oCHiCAx agree believe trans rights genetically...</td>\n",
       "      <td>[ochicax, agree, believe, trans, rights, genet...</td>\n",
       "      <td>[ochicax, agree, believe, trans, rights, genet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42</td>\n",
       "      <td>constantly making fun of the wnba and women's ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>constantly making fun of the wnba and womens s...</td>\n",
       "      <td>constantly making wnba womens sports isnt funny</td>\n",
       "      <td>[constantly, making, wnba, womens, sports, isn...</td>\n",
       "      <td>[constantly, making, wnba, womens, sports, isn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>Funny how things come full circle, @EmilyJones...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Funny how things come full circle EmilyJonesMc...</td>\n",
       "      <td>Funny things come full circle EmilyJonesMcCoy ...</td>\n",
       "      <td>[funny, things, come, full, circle, emilyjones...</td>\n",
       "      <td>[funny, things, come, full, circle, emilyjones...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>42</td>\n",
       "      <td>constantly making fun of the wnba and women's ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>constantly making fun of the wnba and womens s...</td>\n",
       "      <td>constantly making wnba womens sports isnt funny</td>\n",
       "      <td>[constantly, making, wnba, womens, sports, isn...</td>\n",
       "      <td>[constantly, making, wnba, womens, sports, isn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>Winter sports are funny, especially when you’r...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Winter sports are funny especially when you’re...</td>\n",
       "      <td>Winter sports funny especially when you’re get...</td>\n",
       "      <td>[winter, sports, funny, especially, when, you’...</td>\n",
       "      <td>[winter, sports, funny, especially, you’re, ge...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retweetcount                                               text  \\\n",
       "0             0  @TulsiGabbard the biological essentialism of T...   \n",
       "1             1  mighty funny how nobody in black sports media ...   \n",
       "2             0  Tht time I was put in a gc full of barstool sp...   \n",
       "3             1  mighty funny how nobody in black sports media ...   \n",
       "4             0  @DanSoder @katienolan that Andre AD read made ...   \n",
       "5             0  @oCHiCAx I agree. I believe in trans rights 2 ...   \n",
       "6            42  constantly making fun of the wnba and women's ...   \n",
       "7             1  Funny how things come full circle, @EmilyJones...   \n",
       "8            42  constantly making fun of the wnba and women's ...   \n",
       "9             3  Winter sports are funny, especially when you’r...   \n",
       "\n",
       "     hashtags                                  Tweet_punctuation  \\\n",
       "0          []  TulsiGabbard the biological essentialism of TE...   \n",
       "1          []  mighty funny how nobody in black sports media ...   \n",
       "2          []  Tht time I was put in a gc full of barstool sp...   \n",
       "3          []  mighty funny how nobody in black sports media ...   \n",
       "4  ['sports']  DanSoder katienolan that Andre AD read made me...   \n",
       "5          []  oCHiCAx I agree I believe in trans rights  but...   \n",
       "6          []  constantly making fun of the wnba and womens s...   \n",
       "7          []  Funny how things come full circle EmilyJonesMc...   \n",
       "8          []  constantly making fun of the wnba and womens s...   \n",
       "9          []  Winter sports are funny especially when you’re...   \n",
       "\n",
       "                                        Tweet_no_mot  \\\n",
       "0  TulsiGabbard biological essentialism TERFs fun...   \n",
       "1  mighty funny nobody black sports media standin...   \n",
       "2  time full barstool sports type funny accounts ...   \n",
       "3  mighty funny nobody black sports media standin...   \n",
       "4  DanSoder katienolan that Andre read made swerv...   \n",
       "5  oCHiCAx agree believe trans rights genetically...   \n",
       "6    constantly making wnba womens sports isnt funny   \n",
       "7  Funny things come full circle EmilyJonesMcCoy ...   \n",
       "8    constantly making wnba womens sports isnt funny   \n",
       "9  Winter sports funny especially when you’re get...   \n",
       "\n",
       "                                     Tweet_tokenized  \\\n",
       "0  [tulsigabbard, biological, essentialism, terfs...   \n",
       "1  [mighty, funny, nobody, black, sports, media, ...   \n",
       "2  [time, full, barstool, sports, type, funny, ac...   \n",
       "3  [mighty, funny, nobody, black, sports, media, ...   \n",
       "4  [dansoder, katienolan, that, andre, read, made...   \n",
       "5  [ochicax, agree, believe, trans, rights, genet...   \n",
       "6  [constantly, making, wnba, womens, sports, isn...   \n",
       "7  [funny, things, come, full, circle, emilyjones...   \n",
       "8  [constantly, making, wnba, womens, sports, isn...   \n",
       "9  [winter, sports, funny, especially, when, you’...   \n",
       "\n",
       "                                         Tweet_words  \n",
       "0  [tulsigabbard, biological, essentialism, terfs...  \n",
       "1  [mighty, funny, nobody, black, sports, media, ...  \n",
       "2  [time, full, barstool, sports, type, funny, ac...  \n",
       "3  [mighty, funny, nobody, black, sports, media, ...  \n",
       "4  [dansoder, katienolan, andre, read, made, swer...  \n",
       "5  [ochicax, agree, believe, trans, rights, genet...  \n",
       "6  [constantly, making, wnba, womens, sports, isn...  \n",
       "7  [funny, things, come, full, circle, emilyjones...  \n",
       "8  [constantly, making, wnba, womens, sports, isn...  \n",
       "9  [winter, sports, funny, especially, you’re, ge...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "df['Tweet_words'] = df['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le stemming consiste à réduire un mot dans sa forme « racine ». Le but du stemming est de regrouper de nombreuses variantes d’un mot comme un seul et même mot. Par exemple, une fois que l’on applique un stemming sur « Chiens » ou « Chien », le mot résultant est le même. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Tweet_punctuation</th>\n",
       "      <th>Tweet_no_mot</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_words</th>\n",
       "      <th>Tweet_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@TulsiGabbard the biological essentialism of T...</td>\n",
       "      <td>[]</td>\n",
       "      <td>TulsiGabbard the biological essentialism of TE...</td>\n",
       "      <td>TulsiGabbard biological essentialism TERFs fun...</td>\n",
       "      <td>[tulsigabbard, biological, essentialism, terfs...</td>\n",
       "      <td>[tulsigabbard, biological, essentialism, terfs...</td>\n",
       "      <td>[tulsigabbard, biolog, essenti, terf, funni, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>mighty funny nobody black sports media standin...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "      <td>[mighti, funni, nobodi, black, sport, media, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>time full barstool sports type funny accounts ...</td>\n",
       "      <td>[time, full, barstool, sports, type, funny, ac...</td>\n",
       "      <td>[time, full, barstool, sports, type, funny, ac...</td>\n",
       "      <td>[time, full, barstool, sport, type, funni, acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>mighty funny nobody black sports media standin...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "      <td>[mighti, funni, nobodi, black, sport, media, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@DanSoder @katienolan that Andre AD read made ...</td>\n",
       "      <td>['sports']</td>\n",
       "      <td>DanSoder katienolan that Andre AD read made me...</td>\n",
       "      <td>DanSoder katienolan that Andre read made swerv...</td>\n",
       "      <td>[dansoder, katienolan, that, andre, read, made...</td>\n",
       "      <td>[dansoder, katienolan, andre, read, made, swer...</td>\n",
       "      <td>[dansod, katienolan, andr, read, made, swerv, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retweetcount                                               text  \\\n",
       "0             0  @TulsiGabbard the biological essentialism of T...   \n",
       "1             1  mighty funny how nobody in black sports media ...   \n",
       "2             0  Tht time I was put in a gc full of barstool sp...   \n",
       "3             1  mighty funny how nobody in black sports media ...   \n",
       "4             0  @DanSoder @katienolan that Andre AD read made ...   \n",
       "\n",
       "     hashtags                                  Tweet_punctuation  \\\n",
       "0          []  TulsiGabbard the biological essentialism of TE...   \n",
       "1          []  mighty funny how nobody in black sports media ...   \n",
       "2          []  Tht time I was put in a gc full of barstool sp...   \n",
       "3          []  mighty funny how nobody in black sports media ...   \n",
       "4  ['sports']  DanSoder katienolan that Andre AD read made me...   \n",
       "\n",
       "                                        Tweet_no_mot  \\\n",
       "0  TulsiGabbard biological essentialism TERFs fun...   \n",
       "1  mighty funny nobody black sports media standin...   \n",
       "2  time full barstool sports type funny accounts ...   \n",
       "3  mighty funny nobody black sports media standin...   \n",
       "4  DanSoder katienolan that Andre read made swerv...   \n",
       "\n",
       "                                     Tweet_tokenized  \\\n",
       "0  [tulsigabbard, biological, essentialism, terfs...   \n",
       "1  [mighty, funny, nobody, black, sports, media, ...   \n",
       "2  [time, full, barstool, sports, type, funny, ac...   \n",
       "3  [mighty, funny, nobody, black, sports, media, ...   \n",
       "4  [dansoder, katienolan, that, andre, read, made...   \n",
       "\n",
       "                                         Tweet_words  \\\n",
       "0  [tulsigabbard, biological, essentialism, terfs...   \n",
       "1  [mighty, funny, nobody, black, sports, media, ...   \n",
       "2  [time, full, barstool, sports, type, funny, ac...   \n",
       "3  [mighty, funny, nobody, black, sports, media, ...   \n",
       "4  [dansoder, katienolan, andre, read, made, swer...   \n",
       "\n",
       "                                       Tweet_stemmed  \n",
       "0  [tulsigabbard, biolog, essenti, terf, funni, a...  \n",
       "1  [mighti, funni, nobodi, black, sport, media, s...  \n",
       "2  [time, full, barstool, sport, type, funni, acc...  \n",
       "3  [mighti, funni, nobodi, black, sport, media, s...  \n",
       "4  [dansod, katienolan, andr, read, made, swerv, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "def stemming(text):\n",
    "    text = [ps.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "df['Tweet_stemmed'] = df['Tweet_words'].apply(lambda x: stemming(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La stemming et la lemmatisation sont des techniques de normalisation de texte (ou parfois appelées normalisation de mots ) dans le domaine du traitement du langage naturel qui sont utilisées pour préparer du texte, des mots et des documents pour un traitement ultérieur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nouha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweetcount</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>Tweet_punctuation</th>\n",
       "      <th>Tweet_no_mot</th>\n",
       "      <th>Tweet_tokenized</th>\n",
       "      <th>Tweet_words</th>\n",
       "      <th>Tweet_stemmed</th>\n",
       "      <th>Tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@TulsiGabbard the biological essentialism of T...</td>\n",
       "      <td>[]</td>\n",
       "      <td>TulsiGabbard the biological essentialism of TE...</td>\n",
       "      <td>TulsiGabbard biological essentialism TERFs fun...</td>\n",
       "      <td>[tulsigabbard, biological, essentialism, terfs...</td>\n",
       "      <td>[tulsigabbard, biological, essentialism, terfs...</td>\n",
       "      <td>[tulsigabbard, biolog, essenti, terf, funni, a...</td>\n",
       "      <td>[tulsigabbard, biological, essentialism, terfs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>mighty funny nobody black sports media standin...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "      <td>[mighti, funni, nobodi, black, sport, media, s...</td>\n",
       "      <td>[mighty, funny, nobody, black, sport, medium, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Tht time I was put in a gc full of barstool sp...</td>\n",
       "      <td>time full barstool sports type funny accounts ...</td>\n",
       "      <td>[time, full, barstool, sports, type, funny, ac...</td>\n",
       "      <td>[time, full, barstool, sports, type, funny, ac...</td>\n",
       "      <td>[time, full, barstool, sport, type, funni, acc...</td>\n",
       "      <td>[time, full, barstool, sport, type, funny, acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>mighty funny how nobody in black sports media ...</td>\n",
       "      <td>mighty funny nobody black sports media standin...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "      <td>[mighty, funny, nobody, black, sports, media, ...</td>\n",
       "      <td>[mighti, funni, nobodi, black, sport, media, s...</td>\n",
       "      <td>[mighty, funny, nobody, black, sport, medium, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@DanSoder @katienolan that Andre AD read made ...</td>\n",
       "      <td>['sports']</td>\n",
       "      <td>DanSoder katienolan that Andre AD read made me...</td>\n",
       "      <td>DanSoder katienolan that Andre read made swerv...</td>\n",
       "      <td>[dansoder, katienolan, that, andre, read, made...</td>\n",
       "      <td>[dansoder, katienolan, andre, read, made, swer...</td>\n",
       "      <td>[dansod, katienolan, andr, read, made, swerv, ...</td>\n",
       "      <td>[dansoder, katienolan, andre, read, made, swer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   retweetcount                                               text  \\\n",
       "0             0  @TulsiGabbard the biological essentialism of T...   \n",
       "1             1  mighty funny how nobody in black sports media ...   \n",
       "2             0  Tht time I was put in a gc full of barstool sp...   \n",
       "3             1  mighty funny how nobody in black sports media ...   \n",
       "4             0  @DanSoder @katienolan that Andre AD read made ...   \n",
       "\n",
       "     hashtags                                  Tweet_punctuation  \\\n",
       "0          []  TulsiGabbard the biological essentialism of TE...   \n",
       "1          []  mighty funny how nobody in black sports media ...   \n",
       "2          []  Tht time I was put in a gc full of barstool sp...   \n",
       "3          []  mighty funny how nobody in black sports media ...   \n",
       "4  ['sports']  DanSoder katienolan that Andre AD read made me...   \n",
       "\n",
       "                                        Tweet_no_mot  \\\n",
       "0  TulsiGabbard biological essentialism TERFs fun...   \n",
       "1  mighty funny nobody black sports media standin...   \n",
       "2  time full barstool sports type funny accounts ...   \n",
       "3  mighty funny nobody black sports media standin...   \n",
       "4  DanSoder katienolan that Andre read made swerv...   \n",
       "\n",
       "                                     Tweet_tokenized  \\\n",
       "0  [tulsigabbard, biological, essentialism, terfs...   \n",
       "1  [mighty, funny, nobody, black, sports, media, ...   \n",
       "2  [time, full, barstool, sports, type, funny, ac...   \n",
       "3  [mighty, funny, nobody, black, sports, media, ...   \n",
       "4  [dansoder, katienolan, that, andre, read, made...   \n",
       "\n",
       "                                         Tweet_words  \\\n",
       "0  [tulsigabbard, biological, essentialism, terfs...   \n",
       "1  [mighty, funny, nobody, black, sports, media, ...   \n",
       "2  [time, full, barstool, sports, type, funny, ac...   \n",
       "3  [mighty, funny, nobody, black, sports, media, ...   \n",
       "4  [dansoder, katienolan, andre, read, made, swer...   \n",
       "\n",
       "                                       Tweet_stemmed  \\\n",
       "0  [tulsigabbard, biolog, essenti, terf, funni, a...   \n",
       "1  [mighti, funni, nobodi, black, sport, media, s...   \n",
       "2  [time, full, barstool, sport, type, funni, acc...   \n",
       "3  [mighti, funni, nobodi, black, sport, media, s...   \n",
       "4  [dansod, katienolan, andr, read, made, swerv, ...   \n",
       "\n",
       "                                    Tweet_lemmatized  \n",
       "0  [tulsigabbard, biological, essentialism, terfs...  \n",
       "1  [mighty, funny, nobody, black, sport, medium, ...  \n",
       "2  [time, full, barstool, sport, type, funny, acc...  \n",
       "3  [mighty, funny, nobody, black, sport, medium, ...  \n",
       "4  [dansoder, katienolan, andre, read, made, swer...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    text = [wn.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "df['Tweet_lemmatized'] = df['Tweet_words'].apply(lambda x: lemmatizer(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nouha\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df.Tweet_lemmatized.to_csv('Tweets_cleaned.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>['tulsigabbard', 'biological', 'essentialism', 'terfs', 'funny', 'apparently', 'vouch', 'equality', 'insist', 'moment', 'time', 'youre', 'inferior', 'unable', 'compete', 'biological', 'maybe', 'youre', 'good', 'sport']</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['mighty', 'funny', 'nobody', 'black', 'sport'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['time', 'full', 'barstool', 'sport', 'type', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['mighty', 'funny', 'nobody', 'black', 'sport'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['dansoder', 'katienolan', 'andre', 'read', 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['ochicax', 'agree', 'believe', 'trans', 'righ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['constantly', 'making', 'wnba', 'woman', 'spo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>['funny', 'thing', 'come', 'full', 'circle', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>['constantly', 'making', 'wnba', 'woman', 'spo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>['winter', 'sport', 'funny', 'especially', 'yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>['funny', 'thing', 'come', 'full', 'circle', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ['tulsigabbard', 'biological', 'essentialism', 'terfs', 'funny', 'apparently', 'vouch', 'equality', 'insist', 'moment', 'time', 'youre', 'inferior', 'unable', 'compete', 'biological', 'maybe', 'youre', 'good', 'sport']\n",
       "0  ['mighty', 'funny', 'nobody', 'black', 'sport'...                                                                                                                                                                        \n",
       "1  ['time', 'full', 'barstool', 'sport', 'type', ...                                                                                                                                                                        \n",
       "2  ['mighty', 'funny', 'nobody', 'black', 'sport'...                                                                                                                                                                        \n",
       "3  ['dansoder', 'katienolan', 'andre', 'read', 'm...                                                                                                                                                                        \n",
       "4  ['ochicax', 'agree', 'believe', 'trans', 'righ...                                                                                                                                                                        \n",
       "5  ['constantly', 'making', 'wnba', 'woman', 'spo...                                                                                                                                                                        \n",
       "6  ['funny', 'thing', 'come', 'full', 'circle', '...                                                                                                                                                                        \n",
       "7  ['constantly', 'making', 'wnba', 'woman', 'spo...                                                                                                                                                                        \n",
       "8  ['winter', 'sport', 'funny', 'especially', 'yo...                                                                                                                                                                        \n",
       "9  ['funny', 'thing', 'come', 'full', 'circle', '...                                                                                                                                                                        "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tweet_df= pd.read_csv('Tweets_cleaned.csv')\n",
    "new_tweet_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: (2984, 1)\n",
      "Columns are: Index(['['tulsigabbard', 'biological', 'essentialism', 'terfs', 'funny', 'apparently', 'vouch', 'equality', 'insist', 'moment', 'time', 'youre', 'inferior', 'unable', 'compete', 'biological', 'maybe', 'youre', 'good', 'sport']'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Dataset size:',new_tweet_df.shape)\n",
    "print('Columns are:',new_tweet_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorisation\n",
    "- Les données nettoyées en une seule ligne en passant new_tweet_df dans le CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'Tweet_lemmatized'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3025b1ed99e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_tweet_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTweet_lemmatized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5065\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5066\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5067\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'Tweet_lemmatized'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X=cv.fit_transform(new_tweet_df.Tweet_lemmatized)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification des tweets\n",
    "\n",
    "* Cette approche utilise la technique de création d'un ensemble de mots qui peuvent être classés en toute confiance comme  appartenant à une catégorie particulière .\n",
    "* On va Utiliser l’algorithme K-Means pour classer les Tweets en 30 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ********** Nombre de cluster : ************ 3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-0f7f9c04bb2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" ********** Nombre de cluster : ************\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mKmeans\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'k-means++'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_init\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mKmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mwcss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "wcss=[]\n",
    "for i in range(3,30):\n",
    "   \n",
    "    Kmeans=KMeans(n_clusters=i,init='k-means++',max_iter=300,n_init=10,random_state=0,verbose=True)\n",
    "    Kmeans.fit(X)\n",
    "    wcss.append(Kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(3,30),wcss)\n",
    "plt.xlabel('number of clusters')\n",
    "plt.ylabel('word per cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_k=30\n",
    "Kmeans=KMeans(n_clusters=true_k,init='k-means++',n_init=1)\n",
    "Kmeans.fit(X)\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = Kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    print(\"-----------------------\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind])\n",
    "    print()\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "j=0\n",
    "while i<28:\n",
    "    while True: \n",
    "        Y=cv.transform([new_tweet_df.Tweet_lemmatized[j]])\n",
    "        prediction=Kmeans.predict(Y)\n",
    "        if i == prediction:\n",
    "            print(\"Tweet of cluster \"+str(prediction)+\" : \"+df.Tweet_lemmatized[i])\n",
    "            print (\"-----------------------------------------------\")\n",
    "            print(\"\\n\")\n",
    "            j=0\n",
    "            break\n",
    "        j+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
